---
title: "AP聚类"
date: 2023-01-12
katex: true
---

#### 近邻传播聚类

Affinity Propagation聚类简称AP，是给予数据点间的信息传递的一种聚类算法。算法的基本思想是将全部样本看作网络节点，通过网络中各条边的消息传递计算出各样本的聚类中心。聚类过程中，共有两种消息在各节点之间传递，分别是吸引度和归属度，在节点之间的信息的相互传递，最终选出代表完成聚类。AP算法通过不断的迭代过程不断更新每一个节点之间的吸引度和归属度值，直到产生m个高质量的Exampler，同时将其余的数据点分配到相应的聚类中。

#### 算法优点

1. 不需要制定最终聚类族的个数

2. 已有的数据点作为最终的聚类中心，而不是新生成一个族中心。
3. 模型对数据的初始值不敏感。
4. 对初始相似度矩阵数据的对称性没有要求。
5. 相比与k-centers聚类方法，其结果的平方差误差较小。

#### AP聚类重要参数

preference（定义聚类数量）和damping factor（控制算法的收敛效果），实际上和一般的k均值聚类类似，AP聚类也是一个不断迭代的过程。

最终构成两个矩阵$R=[r(i,k)]_{N\times N}$和归属度$A=[a(i,k)]_{N\times N}$

$$
r(i,k)=s(i,k)=\max_{k'\neq k}(a(i,k')+s(i,k'))
$$

$$
a(i,k)=\left\{\begin{aligned}
&\min\{0,r(k,k)+\sum_{i'\notin\{i,k\}}\max(0,r(i',k)\},i\neq k\\
&\sum_{i'\neq k}\max(0,r(i',k)),i=k
\end{aligned}\right.
$$


$$
r(i,k)\leftarrow s(i,k)-\max\{a(i,k')+s(i,k')\}
\\a(i,k)\leftarrow \min\{0,r(k,k)+\sum\max\{0,r(i',k)\}\}
$$

#### 参数解释

$s(i,k)$表示的是$i$和$k$之间的相似性。表示的是k适合作为$i$聚类中心的程度。越大表示越合适。

$s(k,k)$称为参考度(Preference)，是相似度矩阵中横轴和纵轴坐标相同的点，最终的类别数目受初始参考度(Preference)的影响：通常是参考度越大则最终的聚类中心的数目越多。

迭代开始之前所有成为聚类中心的可能性相同，应该设置一个公共值：相似度矩阵的平均值或者相似度矩阵的最小值。

$r(i,k)$称为吸引度，表示$k$对于$i$的吸引度。

$a(i,k)$称为归属度，从候选聚类中心$k$到点$i$。反映的是在考虑其余点之后，而k作为一个整体的合适程度。

```text
1.在给定归属度的条件下，更新相似度矩阵中的每个点的吸引度信息
2.在给定吸引度的条件下，更新每个点的归属度信息
3.对每个点的吸引度信息和归属度信息进行求和，进行决策；判断迭代是否停止
```



### AP聚类优势

传统的聚类算法主要基于划分的聚类算法、基于密度的聚类、基于网络的聚类、基于模型的聚类等。而k-均值聚类算法是基于划分类算法中一个较为典型的方法。

AP聚类的优势在于不需要预设聚类簇数，而认为所有的样本点数据都可以作为聚类中心点的。多次执行AP聚类算法，得到的聚类结果也是一致的。

步骤 1:初始化相似度矩阵 S，输入 curlterNum, maxlterNum, *a*(*i,k*)=0, *λ, δ*=0, stableNum。其中 curlterNum 表示当前已迭代次数;maxlterNum 表示最大迭代次数; *λ* 为阻尼因子，默认为 0.7;*δ* 记录聚类中心是否已达到指定稳定次数 stableNum。



### 在python上的实现

python的sklearn中已经实现了AP算法，可以直接进行调用。

```python
class sklearn.cluster.AffinityPropagation(damping=0.5, max_iter=200, convergence_iter=15, copy=True, preference=None, affinity='euclidean', verbose=False)
```

