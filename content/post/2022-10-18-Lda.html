---
title: "LDA Method And Realization"
author: "Jianqi Huang"
date: "2022-10-18"
output: html_document
toc: true
---


<div id="TOC">

</div>

<div id="lda-and-dimension-reduction" class="section level1">
<h1>LDA and dimension reduction</h1>
<div id="what-is-ldathe-linear-discriminant-analysis" class="section level2">
<h2>what is LDA:the linear discriminant analysis</h2>
<p>LDA is a supervised classification technique that is considered a part of crafting competitive machine learning models.</p>
<p>The original technique was developed in the year 1936 by Ronald A. Fisher and was named Linear Discriminant or Fisher’s Discriminant Analysis. The original Linear Discriminant was described as a two-class technique.
The multi-class version was later generalized by C.R Rao as Multiple Discriminant Analysis. They are all simply referred to as the Linear Discriminant Analysis.</p>
<p>The goal of LDA is to project the feature from a high dimension to a lower-dimensional space so that avoid the dimension catastrophe and also reduce resources and dimensional costs.</p>
</div>
<div id="why-dimension-reduction" class="section level2">
<h2>Why dimension reduction</h2>
<p>In simple terms, they reduce the dimensions in a particular dataset while retaining most of the data.</p>
<p>In many situation, the feature attributes often show the correlation. So some information is redundant that it is unnecessary to use all of the attributes to forecast or classify. so we can use the DR tech to help us reduce the dimension in classifier.</p>
</div>
<div id="loading-the-data" class="section level2">
<h2>loading the data</h2>
<pre class="r"><code>library(MASS)
rm(list=ls())
data&lt;-read.csv(&quot;/Users/a182501/Desktop/大三上/data-mining/read_csv-lda.csv&quot;,header=FALSE,as.is = TRUE,encoding = &quot;UTF-8&quot;)</code></pre>
</div>
<div id="bayes-prior-probablity" class="section level2">
<h2>Bayes prior probablity</h2>
<p>we use the Bayes theory and set the Priori without information.</p>
<pre class="r"><code>condata&lt;-data[1:20,1:4]
grpdata&lt;-data[1:20,5]
(con.sol=lda(condata,grpdata,prior = c(1,1,1)/3))</code></pre>
<pre><code>## Call:
## lda(condata, grpdata, prior = c(1, 1, 1)/3)
## 
## Prior probabilities of groups:
##         1         2         3 
## 0.3333333 0.3333333 0.3333333 
## 
## Group means:
##        V1       V2        V3       V4
## 1 81.7000 16.56667 12.616667 47211.33
## 2 75.8375 13.96250  8.725000 17285.62
## 3 67.1500 11.70000  7.966667  7747.50
## 
## Coefficients of linear discriminants:
##             LD1           LD2
## V1 2.401160e-01  1.597322e-01
## V2 2.175421e-01  3.708095e-01
## V3 2.397799e-01 -4.139835e-01
## V4 6.483192e-05 -5.180125e-05
## 
## Proportion of trace:
##    LD1    LD2 
## 0.9659 0.0341</code></pre>
<pre class="r"><code>ins&lt;-data[21:22,1:4]
pred&lt;-predict(con.sol,ins)
predall&lt;-predict(con.sol,data[1:22,1:4])
df&lt;-cbind(predall$class,predall$x)
df&lt;-as.data.frame(df)
df$V1&lt;-as.factor(df$V1)
ggplot(data = df)+
  geom_point(aes(x=LD1,y=LD2,color=V1))</code></pre>
<p><img src="/en/blog/2022-10-18-Lda_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Finally we transfer the three-dimension feature into two-dimension feature. And we will have the feature function of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span></p>
</div>
</div>
