<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>The Bagging Althorithm Principle and Implementation</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jianqi Huang" />
    <meta name="author" content="Junda Chen" />
    <meta name="date" content="2023-02-18" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/xaringanExtra-banner/banner.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-banner/banner.js"></script>
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <script src="libs/xaringanExtra-webcam/webcam.js"></script>
    <script id="xaringanExtra-webcam-options" type="application/json">{"width":"200","height":"200","margin":"1em"}</script>
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script type="application/json" id="xaringanExtra-editable-docid">{"id":"c344dea1aaec4ae48ada3b05b6f88b97","expires":14}</script>
    <script src="libs/himalaya/himalaya.js"></script>
    <script src="libs/js-cookie/js.cookie.js"></script>
    <link href="libs/editable/editable.css" rel="stylesheet" />
    <script src="libs/editable/editable.js"></script>
    <link href="libs/animate.css/animate.xaringan.css" rel="stylesheet" />
    <link rel="stylesheet" href="zh-CN.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# The Bagging Althorithm Principle and Implementation
]
.author[
### Jianqi Huang
]
.author[
### Junda Chen
]
.institute[
### SME,CUFE
]
.date[
### <br/><br/>18 February 2023
]

---



<script>document.addEventListener('DOMContentLoaded',function(){new xeBanner(JSON.parse('{"exclude":["title-slide"],"position":"top"}'))})</script>
<script>document.addEventListener('DOMContentLoaded',function(){new xeBanner(JSON.parse('{"left":"Data-mining And Multivariate-regression","exclude":["title-slide"],"position":"bottom"}'))})</script>















---
class: middle,center
# Before Bagging 

---
## Resampling

- Evaluate the learning effect from data: the generalization performance.

--

- Just based on data: good performance in this data but not in others.

--

- **Validation Approach**: splitting training set into training set and validation set(or hold set).**it always causes a positive estimate**.

--

- **Re-sampling methods** provide an alternative approach by allowing us to repeatedly fit a model of interest to parts of the training data and test its performance on other parts. 

--

### The Advantage of Bootstrap

- Since observations are replicated in bootstrapping, there tends to be less variability in the error measure compared with k-fold CV(cross validation).

---

## Bootstrap

- A bootstrap sample is a random sample of the data taken with replacement 

- A bootstrap sample is the same size as the original data set from which it was constructed.

![illstation bootstrap](bootstrap-scheme.png)

---

class: middle
## Bootstrap with code

---


.panelset[
 .panel[.panel-name[Python Code]
````python
import numpy as np
np.random.seed(123)
pop = np.random.randint(0,500 , size=1000)
sample = np.random.choice(pop, size=300) #so n=300
sample_mean = []
for _ in range(10000):  #so B=10000
    sample_n = np.random.choice(sample, size=300)
    sample_mean.append(sample_n.mean())
import matplotlib.pyplot as plt
plt.hist(sample_mean)
```
]
.panel[.panel-name[Plot]
&lt;img src="img.png" width="576" /&gt;

]
]




---

.panelset[
 .panel[.panel-name[R Code]

Resample one time 
```r
library(rpart)
library(MASS)
data(Pima.tr) ## load data
Diabetes &lt;- Pima.tr[,8] ## response
X &lt;- Pima.tr[,-8] ## predictor
tree &lt;- rpart(Diabetes ~ ., data=X,
control=rpart.control(xval=10))) ## 10-fold CV
n &lt;- nrow(X)
subsample &lt;- sample(1:n, n , replace=TRUE)
sort(subsample)
tree_boot &lt;- rpart(Diabetes ~ ., data=X, subset=subsample,
control=rpart.control(xval=10))) ## 10-fold CV
```
]
  .panel[.panel-name[Plot]
  
- Doing this twice, we get the two following trees, each fitted on a different
(random) subset of the data.
  ![](tree twice.png)
  ]
]


---

## Bagging(from *B*ootstrap *Agg*regat*ing*)
![](pri2.jpg)

---

### Definition 



- Bootstrap aggregating (Bagging) prediction models is a general method for fitting multiple versions of a prediction model and then combining (or ensembling) them into an aggregated prediction

--

- Bagging is a fairly straight forward algorithm in which b Bootstrap copies of the original training data are created

--

- New predictions are made by averaging the predictions together from the individual base learners.

`$$\widehat{f(X_{bag})}=\widehat{f_1(X)}+\widehat{f_2(X)}+\cdots+\widehat{f_b(X)}$$`
- the `\(\widehat{f(X_{bag})}\)` is bagged prediction.

- The `\(\widehat{f_1(X)},\widehat{f_2(X)},\cdots,\widehat{f_b(X)}\)` are the predictions from the individual base learners.

- Bagging does not always improve upon an individual base learner.

- Bagging works especially well for unstable, high variance base learners

---

## Algorithm

Bagging Tree has the following algorithm. Let `\(\hat Y\)` be a tree(or other predictor) based on samples `\((X_1,Y_1),\cdots,(X_n,Y_n)\)`

--

- Draw indices `\((j_1,\cdots,j_n)\)` from the set `\(\{1,\cdots,n\}\)` with replacement. Fit the tree `\(\hat{Y^*}\)` based on samples

`$$(X_{j1},Y_{j1}),\cdots,(X_{jn},Y_{jn})$$`
--

- Repeat first step B times to obtain 
`$$\hat Y^{*,1},\cdots,\hat Y^{*,B}$$`

--

- Bagged estimator is 
`$$\hat Y_{bag}=\frac{1}{B}\sum_{b=1}^B\hat Y^{*,b}$$`

---

### The Thought in Bagging

- for `\(B\to \infty\)`(many bootstrap samples)
`$$\tilde Y_{Bag}\to E(\hat {Y})$$`


- the aggregation of information in large diverse groups results in decisions that are often better than could have been made by any single member of the group. 

- Empirically, Bagging seems to reduce the variance of `\(\hat{Y}\)` e.g.

`$$E((\hat Y-E[\hat Y])^2)\geq E((\hat Y_{Bag}-E[\hat{Y_{Bag}}])^2)$$`

---

## Proof the conclusion in math
Using `\(\widetilde Y_{Bag}\to E(\hat Y)\)` for `\(B\to \infty\)`

`$$E((\hat Y-E[\hat Y])^2)=E[(Y-\widetilde Y_{Bag}+\widetilde Y_{Bag}-\hat Y)^2]
\\=E[(Y-\widetilde Y_{Bag})]^2+E[(\widetilde Y_{Bag}-\hat Y)^2]
\\\geq E((\hat Y_{Bag}-E[\hat{Y_{Bag}}])^2)$$`

the population bagging estimatoe `\(\widetilde Y_{Bag}\)` thus reduced the squared error loss by eliminating the variance of `\(\hat Y\)` around its mean `\(E(\hat Y)\)`


- For trees, this means that bagging has a very beneficial effect on trees with a large size.



---
class: middle,center,inverse
# The Comparison with different depths 

---

Generating a tree with depth of  `\(d=1\)`

&lt;img src="bagging-var-1.png" width="65%" height="65%" style="display: block; margin: auto;" /&gt;

Bagged stumps `\(\hat Y^{*,b},b=1,2,\cdots,10\)`


&lt;img src="bagging-tree-1.png" width="65%" height="65%" style="display: block; margin: auto;" /&gt;

Bagging leads to a small but not a dramatic improvement.

---

### The Depth `\(d=3\)`
The fit with depth `\(d=3\)` have a poor performance 

&lt;img src="bagging-tree-2.png" width="65%" height="65%" style="display: block; margin: auto;" /&gt;

`\(\hat Y\)` has a high variance(and low bias), bagging leads to a large improvement.

&lt;img src="bagging-var-2.png" width="65%" height="65%" style="display: block; margin: auto;" /&gt;

---
class: middle,inverse
## Out of Bag test error estimation

---
### Out of Bag demonstration 


&lt;img src="dot1.png" width="70%" align=center/&gt;

for each `\(v=1,\cdots,V\)`
- fit `\(Y_{Bag}\)` on the training samples,shown as red dots.
- predicts with this tree the left-out test observations, shown as white dots.

---


Fit `\(Y_{Bag}\)` we construct the `\(V\times B\)` fits on which the origional trees.


&lt;img src="dot2.png" width="85%" align=center/&gt;

--

- the computation is expensive.


--
- The core idea: test on the “unused” data points in each bootstrap iteration to estimate test error.



---

class: middle
To answer this question, we need again a good approximation to the test error (here for the squared error loss function L)
`$$R_{test}:=E(L(Y,\hat Y_{Bag}))$$`
--

If fitting B bootstrap estimates `\(\hat Y^{*,b}\)`, to assess the prediction for `\(i=1\)`, average only over such b, where observation `\(i=1\)` has not been used in fitting `\(\hat Y^{*,b}\)`.

--

Recall that, for B bootstrap samples `\(\hat Y^{*,b}\)`, the bagged estimator at observation `\(i\)` is given by `\(\hat Y_i:=\hat Y_{Bag}(X_i)\)`

`$$\hat Y_i=\frac{1}{B}\sum_{b\in \{1,\cdots,B\}}Y^{*,b}(X_i)$$`
---

class: middle
Instead, let now 
`$$\hat Y_i^{obb}=\frac{1}{|\widetilde B_i|}\sum_{b\in \widetilde B_i}Y^{*,b}(X_i)$$`
where the sum is only taken over the set
`$$\widetilde B_i=\{b:X_i \notin trainingset \}\subseteq \{1,\cdots,B\}$$`
--

The estimate of the error is then computed, as usual,by

`$$\hat R_{test}=\frac{1}{n}\sum_{i=1}^nL(Y_i,\hat Y_i^{oob})$$`

---

### The Relation Between `\(|\widetilde B_i|\)` and B
the probablity `\(\pi^{obb}\)` of an observation NOT being included in a bootstrap sample `\((j_1,\cdots,j_n)\)` (and hence called out-of-bag)is, as all `\(j_k\)` for `\(k=1,\cdots,n\)` are drawn with replacement from `\(\{1,\cdots,n\}\)`

`$$\pi^{obb}=\prod_{i=1}^n(1-\frac{1}{n})^n\stackrel{n\to \infty }{\longrightarrow} exp(-1)\approx 0.367$$`


Hence, `\(E(|\widetilde B_i|)=exp(-1)\cdot B\approx 0.367B\)` ,
for all `\(i=1,\cdots,n\)`


---
class: center,inverse,middle
# Bagging Implement 

---

### Algorithm in `Python`
```python
class Bagging:
     def __init__(self, base_learner, n_learners):
        self.learners = [clone(base_learner) for _ in range(n_learners)]

     def fit(self, X, y):
         for learner in self.learners:
             examples = np.random.choice(
                 np.arange(len(X)), int(len(X)), replace=True)
             learner.fit(X.iloc[examples, :], y.iloc[examples])
             
     def predict(self, X):
         preds = [learner.predict(X) for learner in self.learners]
         return np.array(preds).mean(axis=0) 
# in an easy way to use the algorithm
from sklearn.ensemble import BaggingClassifier
```


---
class:center
## Computing Out-of-Bag Estimation

---

### using `R`
.panelset[
  .panel[.panel-name[R Code]
```r
n &lt;- nrow(Boston)
X &lt;- Boston[,-14]
Y &lt;- Boston[,14]
maxdepth&lt;- 10 # plot the depth d = 3 and d = 5
tree &lt;- rpart(Y ~.,data = X,
              control = rpart.control(maxdepth = maxdepth,minsplit = 2))
plot(tree,margin=.1,uniform=TRUE);text(tree,cex=1.3)
```
  ]
  .panel[.panel-name[Plot]
plot Trees of depth `\(d= 3\)` and `\(d=5\)`

![](boston1,10.png)
]]


---

.panelset[
  .panel[.panel-name[R Code]
```r
B &lt;- 100
prediction_oob &lt;- rep(0,length(Y)) ## vector with oob predictions
numbertrees_oob &lt;- rep(0,length(Y)) ## how many oob trees
for (b in 1:B){ ## loop over bootstrap samples
  subsample &lt;- sample(1:n,n,replace=TRUE) ## "in-bag" samples
  outofbag &lt;- (1:n)[-subsample] ## "out-of-bag" samples ## fit tree on "in-bag" samples
  treeboot &lt;- rpart(Y ~ ., data=X, subset=subsample,
                    control=rpart.control(maxdepth=maxdepth,minsplit=2))
  ## predict on oob-samples
  prediction_oob[outofbag] &lt;- prediction_oob[outofbag] +
    predict(treeboot, newdata=X[outofbag,])
  numbertrees_oob[outofbag] &lt;- numbertrees_oob[outofbag] + 1
}
## final oob-prediction is average across all "out-of-bag" trees
prediction_oob &lt;- prediction_oob / numbertrees_oob
plot(prediction_oob, Y, xlab="PREDICTED", ylab="ACTUAL")
df&lt;-as.data.frame(cbind(prediction_oob,Y))
ggplot(data=df,aes(prediction_oob,Y))+
  geom_point(aes(prediction_oob,Y))+
  geom_smooth(method = 'lm',formula = y ~ x, se = F)
```
  ]
  .panel[.panel-name[Plot]
  
&lt;img src="dep1,10.png" width="1741" /&gt;
  ]
]

---

class: middle
## Python
```python
# import relative packages
from sklearn import datasets,tree
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, cross_val_score
from sklearn.ensemble import BaggingClassifier

```

---

.panelset[
  .panel[.panel-name[Python Code]
```python
X, y = datasets.make_moons(n_samples=500, noise=0.3, random_state=42)
plt.scatter(X[y == 0, 0], X[y == 0, 1])
plt.scatter(X[y == 1, 0], X[y == 1, 1])
plt.show()
bagging_clf = BaggingClassifier(DecisionTreeClassifier(),
                                n_estimators=500,
                                max_samples=100,
                                bootstrap=True,
                                oob_score=True)
bagging_clf.fit(X, y)
bagging_clf.oob_score_ # the bagging obb score 
clf = RandomForestClassifier(random_state=0, oob_score=True)
clf.fit(X, y)  
print(clf.oob_score_) # the RF obb score
cv = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=42)
clf = tree.DecisionTreeClassifier(criterion='entropy', random_state=42)
score = cross_val_score(clf, X, y, cv=cv).mean()
print(score) #the cv score by ID3
clf = tree.DecisionTreeClassifier(criterion='gini', random_state=42)
score = cross_val_score(clf,X, y, cv=cv).mean()
print(score) # the cv score by CART
```
]
.panel[.panel-name[Output]

|        | obb scores|
|:-------|----------:|
|Bagging |      0.916|
|RF      |      0.900|
|ID3     |      0.854|
|CART    |      0.858|
]
]


---

class: middle
- Decision tree is unstable, linear regression is stable
![](decision-linear.png)

---
# References
- Efron, Bradley, and Robert Tibshirani. 1986. “Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy.” Statistical Science. JSTOR, 54–75.

- Therneau, Terry M, Elizabeth J Atkinson, and others. 1997. “An Introduction to Recursive Partitioning Using the RPART Routines.” Mayo Foundation.

- https://c.d2l.ai/stanford-cs329p/

- http://www.stats.ox.ac.uk/~teh/datamining.html

- https://bradleyboehmke.github.io/HOML/bagging.html

- https://en.wikipedia.org/wiki/Bootstrapping_(statistics)

- https://en.wikipedia.org/wiki/Bootstrap_aggregating

---

background-image: url('george.jpg')
background-size: cover


---

class: middle,center
# Thank you for your watching!




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
